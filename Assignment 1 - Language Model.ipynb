{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32bfd4a0-4f58-43b2-835f-219916fdb6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to ./movie_review_dataset/\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to the zip file\n",
    "zip_file_path = './archive.zip'\n",
    "# Path to store the extracted data\n",
    "extract_dir = './movie_review_dataset/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f'Files extracted to {extract_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3fb987-4224-414f-95bf-59c930477df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_file_path = './movie_review_dataset/IMDB Dataset.csv'\n",
    "# Read csv\n",
    "df = pd.read_csv(csv_file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365c6157-1f2a-4680-890c-17d406583b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        One of the other reviewers has mentioned that ...\n",
      "1        A wonderful little production. <br /><br />The...\n",
      "2        I thought this was a wonderful way to spend ti...\n",
      "3        Basically there's a family where a little boy ...\n",
      "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
      "                               ...                        \n",
      "49995    I thought this movie did a down right good job...\n",
      "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
      "49997    I am a Catholic taught in parochial elementary...\n",
      "49998    I'm going to have to disagree with the previou...\n",
      "49999    No one expects the Star Trek movies to be high...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Extracting just the reviews\n",
    "print(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88faf3a-2cab-41c2-9c45-a5bd6119ffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\akaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing NLTK libraries\n",
    "import nltk\n",
    "# Loading punkt library\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7b343c-f226-4e81-9125-c2fc7cdbe132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74076b6c-755f-463a-b0bd-1ca32a05657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "380\n"
     ]
    }
   ],
   "source": [
    "def tokenize_each(review):\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['review'].apply(tokenize_each)\n",
    "print(len(df['tokens']))\n",
    "print(len(df['tokens'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9aa93e-0c38-441a-901f-af81ae950bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "all_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d79a48f6-a4d3-4e84-809c-c82ae4d19429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def tokenize_with_normalization_steps(review):\n",
    "    review = re.sub(r'<br /><br />', ' ', review)\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "\n",
    "    # Text normalization steps as practiced in \"Basic Text Transformation\" notebook\n",
    "    # Making everything lower case\n",
    "    normalized_tokens = [re.sub(r'\\W+', '', token.lower()) for token in tokens]\n",
    "    # print(len(norrmalized_tokens))\n",
    "    \n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    normalized_tokens = [normalized_tokens for normalized_tokens in normalized_tokens if len(normalized_tokens) > 1]\n",
    "    \n",
    "    # Remove numbers\n",
    "    normalized_tokens = [normalized_tokens for normalized_tokens in normalized_tokens if not normalized_tokens.isnumeric()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    normalized_tokens = [normalized_tokens for normalized_tokens in normalized_tokens if normalized_tokens not in all_stopwords]\n",
    "\n",
    "    return normalized_tokens\n",
    "\n",
    "df['normalized_tokens'] = df['review'].apply(tokenize_with_normalization_steps)\n",
    "print(len(df['normalized_tokens']))\n",
    "print(len(df['normalized_tokens'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72fe3ede-651d-428e-9796-eff2a6d8b25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'Oz', 'episode', 'you', \"'ll\", 'be', 'hooked', '.', 'They', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'first', 'thing', 'that', 'struck', 'me', 'about', 'Oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'GO', '.', 'Trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', '.', 'This', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', '.', 'Its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'It', 'is', 'called', 'OZ', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary', '.', 'It', 'focuses', 'mainly', 'on', 'Emerald', 'City', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'Em', 'City', 'is', 'home', 'to', 'many', '..', 'Aryans', ',', 'Muslims', ',', 'gangstas', ',', 'Latinos', ',', 'Christians', ',', 'Italians', ',', 'Irish', 'and', 'more', '....', 'so', 'scuffles', ',', 'death', 'stares', ',', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', \"n't\", 'dare', '.', 'Forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '...', 'OZ', 'does', \"n't\", 'mess', 'around', '.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'I', 'could', \"n't\", 'say', 'I', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'I', 'watched', 'more', ',', 'I', 'developed', 'a', 'taste', 'for', 'Oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', '.', 'Not', 'just', 'violence', ',', 'but', 'injustice', '(', 'crooked', 'guards', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'mannered', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', ')', 'Watching', 'Oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '....', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n"
     ]
    }
   ],
   "source": [
    "print(df['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2357dd46-727b-4a2b-96bd-e0c8e16c4a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'reviewers', 'mentioned', 'watching', 'oz', 'episode', 'hooked', 'right', 'exactly', 'happened', 'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focuses', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'never', 'far', 'away', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'would', 'nt', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'nt', 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'could', 'nt', 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guards', 'sold', 'nickel', 'inmates', 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing', 'thats', 'get', 'touch', 'darker', 'side']\n"
     ]
    }
   ],
   "source": [
    "print(df['normalized_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ca8e116-fdc9-493c-9791-a189f762e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of word tokens in the database before normalization steps: 13974186\n",
      "Vocabulary size (number of unique words) of the dataset: 194756\n"
     ]
    }
   ],
   "source": [
    "total_tokens = [token for token_list in df['tokens'] for token in token_list]\n",
    "\n",
    "print(\"Total number of word tokens in the database before normalization steps: \" + str(len(total_tokens)))\n",
    "\n",
    "print(\"Vocabulary size (number of unique words) of the dataset: \" + str(len(set(total_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf0bff68-c592-4c24-b2c6-638cf1b0db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of word tokens in the database after normalization steps: 5910313\n",
      "Vocabulary size (number of unique words) of the dataset after normalization steps: 136431\n"
     ]
    }
   ],
   "source": [
    "total_tokens_normalized = [token for token_list in df['normalized_tokens'] for token in token_list]\n",
    "\n",
    "print(\"Total number of word tokens in the database after normalization steps: \" + str(len(total_tokens_normalized)))\n",
    "\n",
    "print(\"Vocabulary size (number of unique words) of the dataset after normalization steps: \" + str(len(set(total_tokens_normalized))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2592890a-2dc1-4710-9aca-ff7975311418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "from nltk.util import trigrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "def get_top_10_bigrams(tokens):\n",
    "    bi_grams = list(bigrams(tokens))\n",
    "    bi_gram_freq = FreqDist(bi_grams)\n",
    "    return bi_gram_freq.most_common(10)\n",
    "\n",
    "def get_top_10_trigrams(tokens):\n",
    "    tri_grams = list(trigrams(tokens))\n",
    "    tri_gram_freq = FreqDist(tri_grams)\n",
    "\n",
    "    return tri_gram_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8c2c886-db73-489b-96d5-51bc73c89d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = df[df['sentiment'] == 'positive']['normalized_tokens']\n",
    "negative_reviews = df[df['sentiment'] == 'negative']['normalized_tokens']\n",
    "\n",
    "top_10_bigrams_pos = get_top_10_bigrams([token for token_list in positive_reviews for token in token_list])\n",
    "top_10_trigrams_pos = get_top_10_trigrams([token for token_list in positive_reviews for token in token_list])\n",
    "top_10_bigrams_neg = get_top_10_bigrams([token for token_list in negative_reviews for token in token_list])\n",
    "top_10_trigrams_neg = get_top_10_trigrams([token for token_list in negative_reviews for token in token_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c0721f-f658-4403-a731-cb7ca9f33508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 bigrams in the reviews tagged positive: [(('ca', 'nt'), 2858), (('one', 'best'), 1662), (('nt', 'know'), 1241), (('wo', 'nt'), 1215), (('even', 'though'), 1092), (('ever', 'seen'), 964), (('could', 'nt'), 942), (('first', 'time'), 925), (('new', 'york'), 846), (('nt', 'get'), 815)]\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "Top 10 trigrams in the reviews tagged positive: [(('ca', 'nt', 'help'), 222), (('new', 'york', 'city'), 194), (('ca', 'nt', 'wait'), 172), (('world', 'war', 'ii'), 158), (('one', 'best', 'movies'), 144), (('based', 'true', 'story'), 133), (('movie', 'ever', 'seen'), 132), (('ca', 'nt', 'get'), 131), (('one', 'best', 'films'), 131), (('ca', 'nt', 'say'), 126)]\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "Top 10 bigrams in the reviews tagged negative: [(('ca', 'nt'), 4172), (('nt', 'even'), 2233), (('could', 'nt'), 2096), (('ever', 'seen'), 1725), (('nt', 'know'), 1675), (('waste', 'time'), 1427), (('special', 'effects'), 1413), (('would', 'nt'), 1348), (('movie', 'nt'), 1266), (('looks', 'like'), 1231)]\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "Top 10 trigrams in the reviews tagged negative: [(('worst', 'movie', 'ever'), 456), (('movie', 'ever', 'seen'), 392), (('nt', 'waste', 'time'), 389), (('ca', 'nt', 'believe'), 369), (('one', 'worst', 'movies'), 309), (('worst', 'movies', 'ever'), 280), (('movies', 'ever', 'seen'), 265), (('ca', 'nt', 'even'), 242), (('worst', 'film', 'ever'), 199), (('nt', 'make', 'sense'), 199)]\n",
      "\n",
      "****************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 bigrams in the reviews tagged positive: \" + str(top_10_bigrams_pos))\n",
    "print(\"\\n****************************************************************\\n\")\n",
    "print(\"Top 10 trigrams in the reviews tagged positive: \" + str(top_10_trigrams_pos))\n",
    "print(\"\\n****************************************************************\\n\")\n",
    "print(\"Top 10 bigrams in the reviews tagged negative: \" + str(top_10_bigrams_neg))\n",
    "print(\"\\n****************************************************************\\n\")\n",
    "print(\"Top 10 trigrams in the reviews tagged negative: \" + str(top_10_trigrams_neg))\n",
    "print(\"\\n****************************************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70bccc5a-3edc-4d95-8dd8-1701dcef5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_prob(w1, w2, w3, tokens, vocab_size):\n",
    "    bi_grams = list(bigrams(tokens))\n",
    "    bi_gram_freq = FreqDist(bi_grams)\n",
    "    \n",
    "    tri_grams = list(trigrams(tokens))\n",
    "    tri_gram_freq = FreqDist(tri_grams)\n",
    "\n",
    "    curr_bigram = (w1, w2)\n",
    "    curr_trigram = (w1, w2, w3)\n",
    "\n",
    "    # Adding 1 for laplace smoothing\n",
    "    curr_bigram_freq = bi_gram_freq[curr_bigram] + 1\n",
    "    curr_trigram_freq = tri_gram_freq[curr_trigram] + 1\n",
    "\n",
    "    # Probability formula\n",
    "    basic_probability = curr_trigram_freq/(curr_bigram_freq + vocab_size)\n",
    "    log_probability = math.log2(basic_probability)\n",
    "    return (log_probability, basic_probability)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57a319c6-d2cd-4955-b6d5-c8ec54323f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic probability is: 0.003397442440326038\n",
      "Log probability is: -8.20133517568844\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "\n",
    "trigram_test_1 = (\"worst\", \"movie\", \"ever\")\n",
    "trigram_test_2 = (\"new\", \"york\", \"city\")\n",
    "trigram_test_3 = (\"hard\", \"to\", \"watch\")\n",
    "trigram_test_4 = (\"wasted\", \"my\", \"time\")\n",
    "trigram_test_5 = (\"based\", \"true\", \"story\")\n",
    "\n",
    "w1, w2, w3 = trigram_test_1\n",
    "\n",
    "log_probability, basic_probability = calc_prob(w1, w2, w3, total_tokens_normalized, len(set(total_tokens_normalized)))\n",
    "\n",
    "print(\"Basic probability is: \" + str(basic_probability))\n",
    "print(\"Log probability is: \" + str(log_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b17f288d-161b-412e-ac34-7dc68c446476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic probability for trigram ('worst', 'movie', 'ever') is: 0.003397442440326038\n",
      "And its log probability is: -8.20133517568844\n",
      "Basic probability for trigram ('new', 'york', 'city') is: 0.001996602146165798\n",
      "And its log probability is: -8.968237402524753\n",
      "Basic probability for trigram ('hard', 'to', 'watch') is: 7.32965873108948e-06\n",
      "And its log probability is: -17.057822541282476\n",
      "Basic probability for trigram ('wasted', 'my', 'time') is: 7.32965873108948e-06\n",
      "And its log probability is: -17.057822541282476\n",
      "Basic probability for trigram ('based', 'true', 'story') is: 0.0012952988700895732\n",
      "And its log probability is: -9.592499268875036\n"
     ]
    }
   ],
   "source": [
    "test_array = [trigram_test_1, trigram_test_2, trigram_test_3, trigram_test_4, trigram_test_5]\n",
    "for test_case in test_array:\n",
    "    w1, w2, w3 = test_case\n",
    "    log_probability, basic_probability = calc_prob(w1, w2, w3, total_tokens_normalized, len(set(total_tokens_normalized)))\n",
    "    print(\"Basic probability for trigram \" + str(test_case) + \" is: \" + str(basic_probability))\n",
    "    print(\"And its log probability is: \" + str(log_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53fdad-096c-49d1-af21-ccd963cf3254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The number of word tokens in the database. (1 point)\n",
    "Ans: \n",
    "- Total number of word tokens in the database before normalization steps: 13974186\n",
    "- Total number of word tokens in the database after normalization steps: 5910313\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2cf2c-32f4-4f48-a953-f52eb553e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Vocabulary size (number of unique words) of the dataset. (1 point)\n",
    "Ans:\n",
    "- Vocabulary size (number of unique words) of the dataset: 194756\n",
    "- Vocabulary size (number of unique words) of the dataset after normalization steps: 136431\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835e66f-732b-456a-884b-0a0e28b4632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Top ten bigrams and trigrams from positive and negative review sets, including the frequencies. (2 points)\n",
    "Ans:\n",
    " - Top 10 bigrams in the reviews tagged positive: [(('ca', 'nt'), 2858), (('one', 'best'), 1662), (('nt', 'know'), 1241), \n",
    " (('wo', 'nt'), 1215), (('even', 'though'), 1092), (('ever', 'seen'), 964), (('could', 'nt'), 942), \n",
    " (('first', 'time'), 925), (('new', 'york'), 846), (('nt', 'get'), 815)]\n",
    "\n",
    "****************************************************************\n",
    "\n",
    " - Top 10 trigrams in the reviews tagged positive: [(('ca', 'nt', 'help'), 222), (('new', 'york', 'city'), 194), \n",
    " (('ca', 'nt', 'wait'), 172), (('world', 'war', 'ii'), 158), (('one', 'best', 'movies'), 144), \n",
    " (('based', 'true', 'story'), 133), (('movie', 'ever', 'seen'), 132), (('ca', 'nt', 'get'), 131), \n",
    " (('one', 'best', 'films'), 131), (('ca', 'nt', 'say'), 126)]\n",
    "\n",
    "****************************************************************\n",
    "\n",
    " - Top 10 bigrams in the reviews tagged negative: [(('ca', 'nt'), 4172), (('nt', 'even'), 2233), \n",
    " (('could', 'nt'), 2096), (('ever', 'seen'), 1725), (('nt', 'know'), 1675), (('waste', 'time'), 1427), \n",
    " (('special', 'effects'), 1413), (('would', 'nt'), 1348), (('movie', 'nt'), 1266), (('looks', 'like'), 1231)]\n",
    "\n",
    "****************************************************************\n",
    "\n",
    " - Top 10 trigrams in the reviews tagged negative: [(('worst', 'movie', 'ever'), 456), (('movie', 'ever', 'seen'), 392), \n",
    "(('nt', 'waste', 'time'), 389), (('ca', 'nt', 'believe'), 369), (('one', 'worst', 'movies'), 309), \n",
    "(('worst', 'movies', 'ever'), 280), (('movies', 'ever', 'seen'), 265), (('ca', 'nt', 'even'), 242), \n",
    "(('worst', 'film', 'ever'), 199), (('nt', 'make', 'sense'), 199)]\n",
    "\n",
    "****************************************************************\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf95fd3-195f-4ed7-b9c2-01e6b79fca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a sequence of three words (w1,w2,w3), would compute the probability of the third word using trigram language model p(w3|w1,w2).\n",
    "If you're using log-probabilities, use base 2 for computing logs. (5 points)\n",
    "Ans:\n",
    " - calc_prob(w1, w2, w3, tokens, vocab_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037bdb3-9d63-4f79-8208-e73935737a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Five test cases (sequence of three words) showing output from your trigram language model. (2 points)\n",
    "Ans:\n",
    " - Test cases:\n",
    "    1) trigram_test_1 = (\"worst\", \"movie\", \"ever\")\n",
    "    2) trigram_test_2 = (\"new\", \"york\", \"city\")\n",
    "    3) trigram_test_3 = (\"hard\", \"to\", \"watch\")\n",
    "    4) trigram_test_4 = (\"wasted\", \"my\", \"time\")\n",
    "    5) trigram_test_5 = (\"based\", \"true\", \"story\")\n",
    " - Results:\n",
    "    1) Basic probability for trigram ('worst', 'movie', 'ever') is: 0.003397442440326038\n",
    "        And its log probability is: -8.20133517568844\n",
    "    2) Basic probability for trigram ('new', 'york', 'city') is: 0.001996602146165798\n",
    "        And its log probability is: -8.968237402524753\n",
    "    3) Basic probability for trigram ('hard', 'to', 'watch') is: 7.32965873108948e-06\n",
    "        And its log probability is: -17.057822541282476\n",
    "    4) Basic probability for trigram ('wasted', 'my', 'time') is: 7.32965873108948e-06\n",
    "        And its log probability is: -17.057822541282476\n",
    "    5) Basic probability for trigram ('based', 'true', 'story') is: 0.0012952988700895732\n",
    "        And its log probability is: -9.592499268875036\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
